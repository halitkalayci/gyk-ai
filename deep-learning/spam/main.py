import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer

#df = pd.read_csv("spam.csv", encoding_errors='ignore')
df = pd.read_csv("spam.csv", encoding='latin1')
df = df[['v1','v2']]
df.columns = ["label","text"]


label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['label'])

# Eƒüitim - Etiket
texts = df['text'].values
labels = df['label'].values

X_train, X_test, y_train, y_test = train_test_split(texts,labels,test_size=0.2, random_state=42)

# Tokenizer -> verilen bir metni daha k√º√ß√ºk par√ßalara (token) ayƒ±ran bir yapƒ±dƒ±r.

# Ben, sabah kahve i√ßtim. -> Tokenlara ayƒ±rma ve sayƒ± atama. 
# [ {"Ben"->1}, {"Sabah"->2}, {"Kahve"->3}, {"ƒ∞√ßtim"->4}]

# Subword -> Playing -> play->2 ##ing->3 [playfully -> play, ##ful, ##ly]
# Fantastic => [fan, ##tas ##tic]
# gel -> gelemediklerimizden


# Ben sabah kahve i√ßtim, sonra √∂ƒülen tekrar kahve i√ßtim.
# [1,2,3,4,5,6,7,3,4]

# Ben sabah kahve i√ßtim, √ºzerimde kahve renk bir ti≈ü√∂rt var.
# context-aware (BERT) -> Kullanƒ±m noktasƒ±nƒ±n √∂ncesi ve sonrasƒ±na bakarak anlam √ßƒ±kartƒ±lƒ±r.

# Merhaba üòÇ
# Merhaba üò°

# num_words => maximum tutalacak kelime sayƒ±sƒ±
# oov_token => Bilinmeyen kelimeler i√ßin √∂zel token <OOV>

sentences = ["Ben kahve i√ßtim.","Kahve √ßok g√ºzeldi."]
tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')

tokenizer.fit_on_texts(sentences)
print(tokenizer.word_index)
# {'<OOV>': 1, 'kahve': 2, 'ben': 3, 'i√ßtim': 4, '√ßok': 5, 'g√ºzeldi': 6}

test_sentences = ["Ben kahve i√ßmedim."]
sequences = tokenizer.texts_to_sequences(test_sentences)
print(sequences)