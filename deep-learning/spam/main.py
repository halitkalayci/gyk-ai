import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf

#df = pd.read_csv("spam.csv", encoding_errors='ignore')
df = pd.read_csv("spam.csv", encoding='latin1')
df = df[['v1','v2']]
df.columns = ["label","text"]


label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['label'])

# EÄŸitim - Etiket
texts = df['text'].values
labels = df['label'].values

X_train, X_test, y_train, y_test = train_test_split(texts,labels,test_size=0.2, random_state=42)

tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train) # sÃ¶zlÃ¼ÄŸÃ¼ oluÅŸturma.


X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq  = tokenizer.texts_to_sequences(X_test)


X_train_pad = pad_sequences(X_train_seq, maxlen=100, padding='post')
X_test_pad = pad_sequences(X_test_seq, maxlen=100, padding='post')


model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=100), # kelimeleri vektÃ¶rlere Ã§evirir
    # RNN => "Ben kahve iÃ§tim" => Ben,kahve,iÃ§tim -> Ali okula gitti. Sonra geldi.
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)), # LSTM (Long Short-Term Memory)=> GRU,SimpleRNN
    # LSTM'den gelen karmaÅŸÄ±k temsili biraz daha "yoÄŸunlaÅŸtÄ±rmak" iÃ§in.
    # Tam BaÄŸlantÄ±lÄ± Katman
    tf.keras.layers.Dense(32, activation='relu'),
    # Ezberlemeyi Ã¶nlemek (Overfitting) %50 oranÄ±nda rastgele nÃ¶ronlarÄ± eÄŸitimsiz bÄ±rakÄ±r.
    tf.keras.layers.Dropout(0.5),
    # 1 Ã§Ä±ktÄ± oldugu iÃ§in Dense(1)
    tf.keras.layers.Dense(1, activation="sigmoid") # sigmoid => 0-1 arasÄ±nda olasÄ±lÄ±k Ã¼retir (0.94 -> %94 spam)
])

# spam-deÄŸil
# tÃ¼rÃ¼, cins -> Memeli-Kedi



# Padding ["0, 15, 16, 18", "15, 17, 19, 20"] (Derin Ã¶ÄŸrenme modelleri her girdiyi sabit uzunlukta bekler.)
# pre-post => 0'Ä± Ã¶ne mi arkaya mÄ± ekleyelim?
#
# maxlen = Ã‡ok uzun cÃ¼mleleri istenilen maksimum uzunluÄŸa gÃ¶re kesmek iÃ§in.

#bilmediklerim->1
#ben->3
#kahve->2

# 3-2-1

# Tokenizer -> verilen bir metni daha kÃ¼Ã§Ã¼k parÃ§alara (token) ayÄ±ran bir yapÄ±dÄ±r.

# Ben, sabah kahve iÃ§tim. -> Tokenlara ayÄ±rma ve sayÄ± atama. 
# [ {"Ben"->1}, {"Sabah"->2}, {"Kahve"->3}, {"Ä°Ã§tim"->4}]

# Subword -> Playing -> play->2 ##ing->3 [playfully -> play, ##ful, ##ly]
# Fantastic => [fan, ##tas ##tic]
# gel -> gelemediklerimizden


# Ben sabah kahve iÃ§tim, sonra Ã¶ÄŸlen tekrar kahve iÃ§tim.
# [1,2,3,4,5,6,7,3,4]

# Ben sabah kahve iÃ§tim, Ã¼zerimde kahve renk bir tiÅŸÃ¶rt var.
# context-aware (BERT) -> KullanÄ±m noktasÄ±nÄ±n Ã¶ncesi ve sonrasÄ±na bakarak anlam Ã§Ä±kartÄ±lÄ±r.

# Merhaba ðŸ˜‚
# Merhaba ðŸ˜¡

# num_words => maximum tutalacak kelime sayÄ±sÄ±
# oov_token => Bilinmeyen kelimeler iÃ§in Ã¶zel token <OOV>